{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "tee = KeyedVectors.load_word2vec_format(datapath(\"/home/cremaschi/fede/teeentity:s100:w5type:s100:w5\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacebad_chars = [\"-\", \"'\", \"(\", \")\", \"·\", \"1\", \",\", \"ʻ\", \"–\", \"ø\", \"×\", \"/\", \"2\", \"’\", \"æ\", \"ł\", \"&\", \"ı\", \"5\"]\n",
    "replacebad_chars = [\"-\", \"'\", \"(\", \")\", \"·\", \"–\", \"ø\", \"×\", \"/\", \"2\"]\n",
    "\n",
    "def replace_chars(string):\n",
    "    new_l = string\n",
    "    for char in replacebad_chars:\n",
    "        new_l = new_l.replace(char, \"\")\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([k for k in tee.wv.vocab])\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tensorflow #import the module here, so that it can be reloaded.\n",
    "importlib.reload(tensorflow)\n",
    "importlib.reload(os)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "testing = []\n",
    "\n",
    "with open(\"setting_one_training\", \"r\") as file_tp:\n",
    "    for line in file_tp:\n",
    "        a, b, c = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        training.append(((a),b, c))\n",
    "        \n",
    "with open(\"setting_one_testing\", \"r\") as file_tp:\n",
    "    for line in file_tp:\n",
    "        a, b, c = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        testing.append(((a),b, c))\n",
    "        \n",
    "        \n",
    "entities_to_train_on = set(map(lambda x : x[0], training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "testing = []\n",
    "\n",
    "with open(\"setting_two_training_big\", \"r\") as file_tp:\n",
    "    for line in file_tp:\n",
    "        a, b, c = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        training.append(((a),b, c))\n",
    "        \n",
    "with open(\"setting_two_testing_big\", \"r\") as file_tp:\n",
    "    for line in file_tp:\n",
    "        a, b, c = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        testing.append(((a),b, c))\n",
    "        \n",
    "        \n",
    "entities_to_train_on = set(map(lambda x : x[0], training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "testing = []\n",
    "\n",
    "with open(\"setting_three_training\", \"r\") as file_tp:\n",
    "    for line in file_tp:\n",
    "        a, b, c = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        training.append(((a),b, c))\n",
    "        \n",
    "with open(\"setting_three_testing\", \"r\") as file_tp:\n",
    "    for line in file_tp:\n",
    "        a, b, c = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        testing.append(((a),b, c))\n",
    "        \n",
    "        \n",
    "entities_to_train_on = set(map(lambda x : x[0], training))#.union(set(map(lambda x : x[0], testing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list([\"Animal\", \"Fungus\", \"Mammal\", \"Plant\", \"Bacteria\", \"Eukaryote\", \"Species\"])\n",
    "def analyze_dataset(training, testing):\n",
    "    len_training = len(training)\n",
    "    len_testing = len(testing)\n",
    "    for a in classes:\n",
    "        a_in_training = list(filter(lambda x : x[1] == a, training))\n",
    "        a_in_testing = list(filter(lambda x : x[1] == a, testing))\n",
    "        \n",
    "        pos_in_training = float(len(list(filter(lambda x : x[1] == a and int(x[2]) == 1, training))))\n",
    "        pos_in_testing = float(len(list(filter(lambda x : x[1] == a and int(x[2]) == 1, testing))))\n",
    "        \n",
    "        print(a, \"{0:.2f}\".format((pos_in_training/len(a_in_training)))\n",
    "                                  , \"{0:.2f}\".format(pos_in_testing/len(a_in_testing)))\n",
    "    print(len_training, len_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(training,testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltnw._reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.basicConfig = logging.basicConfig(level=logging.DEBUG)\n",
    "import logictensornetworks_wrapper as ltnw\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logictensornetworks as ltn\n",
    "import itertools\n",
    "import argparse\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "ltn.LAYERS = 20\n",
    "ltn.BIAS_factor = -1e-5\n",
    "#ltn.set_universal_aggreg(\"mean\")\n",
    "ltn.set_tnorm(\"luk\")\n",
    "ltn.set_universal_aggreg(\"hmean\")\n",
    "\n",
    "ltnw.predicate(\"Plant\", embedding_size)\n",
    "ltnw.predicate(\"Eukaryote\",embedding_size)\n",
    "ltnw.predicate(\"Species\",embedding_size)\n",
    "ltnw.predicate(\"Mammal\",embedding_size)\n",
    "ltnw.predicate(\"Animal\",embedding_size)\n",
    "ltnw.predicate(\"Fungus\",embedding_size)\n",
    "ltnw.predicate(\"Bacteria\",embedding_size)\n",
    "\n",
    "replacebad_chars = [\"-\", \"'\", \"(\", \")\", \"·\", \"–\", \"ø\", \"×\", \"/\", \"2\"]\n",
    "\n",
    "for l in (entities_to_train_on):\n",
    "    new_l = l\n",
    "    for char in replacebad_chars:\n",
    "        new_l = new_l.replace(char, \"\")\n",
    "    #ltnw.constant(new_l, min_value=[0.] * embedding_size, max_value=[1.] * embedding_size)\n",
    "    ltnw.constant(new_l, tee[l][100:])\n",
    "\n",
    "for index, data in enumerate(training):\n",
    "    new_l = data[0]\n",
    "    c = data[1]\n",
    "    #print(data[0])\n",
    "    for char in replacebad_chars:\n",
    "        new_l = new_l.replace(char, \"\")\n",
    "    \n",
    "    string = c  +\"(\" + new_l + \")\"\n",
    "    if data[2] == '0':\n",
    "        string = \"~\" + string\n",
    "    #logging.info(string)\n",
    "    m = ltnw.axiom(string)\n",
    "    if index%1000==0:\n",
    "        print(index)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = np.array(list(map(lambda x : x[100:], tee[entities_to_train_on])))\n",
    "\n",
    "ltnw.variable(\"?a\",tensor_data);\n",
    "#ltnw.variable(\"?a\",tf.concat(list(ltnw.CONSTANTS.values()),axis=0))\n",
    "ltnw.axiom(\"forall ?a: Plant(?a) -> Eukaryote(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Animal(?a) -> Eukaryote(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Fungus(?a) -> Eukaryote(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Bacteria(?a) -> Species(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Mammal(?a) -> Animal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Eukaryote(?a) -> Species(?a)\")\n",
    "\n",
    "ltnw.axiom(\"forall ?a: Plant(?a) -> ~Mammal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Plant(?a) -> ~Animal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Plant(?a) -> ~Fungus(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Plant(?a) -> ~Bacteria(?a)\")\n",
    "\n",
    "ltnw.axiom(\"forall ?a: Mammal(?a) -> ~Plant(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Mammal(?a) -> ~Fungus(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Mammal(?a) -> ~Bacteria(?a)\")\n",
    "\n",
    "ltnw.axiom(\"forall ?a: Bacteria(?a) -> ~Mammal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Bacteria(?a) -> ~Eukaryote(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Bacteria(?a) -> ~Animal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Bacteria(?a) -> ~Plant(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Bacteria(?a) -> ~Fungus(?a)\")\n",
    "\n",
    "ltnw.axiom(\"forall ?a: Fungus(?a) -> ~Mammal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Fungus(?a) -> ~Animal(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Fungus(?a) -> ~Plant(?a)\")\n",
    "ltnw.axiom(\"forall ?a: Fungus(?a) -> ~Bacteria(?a)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltnw.initialize_knowledgebase(\n",
    "    optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01,decay=0.9),\n",
    "    formula_aggregator=lambda *x: 1./tf.reduce_mean(1./tf.concat(x,axis=0)))\n",
    "\n",
    "# Train the KB\n",
    "sat_level = ltnw.train(max_epochs=10000, track_sat_levels=10, sat_level_epsilon=.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_n = list(map(lambda k : k[1] + \"(\" + replace_chars(k[0]) + \")\", testing))\n",
    "testing_costant = set(map(lambda x : x[0], testing))\n",
    "results = ltnw.ask_m(testing_n)\n",
    "conca = list(zip(testing, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in (testing_costant):\n",
    "    new_l = l\n",
    "    for char in replacebad_chars:\n",
    "        new_l = new_l.replace(char, \"\")\n",
    "    #ltnw.constant(new_l, min_value=[0.] * embedding_size, max_value=[1.] * embedding_size)\n",
    "    ltnw.constant(new_l, tee[l][100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "classes = set([\"Plant\", \"Eukaryote\", \"Species\", \"Fungus\", \"Bacteria\", \"Animal\", \"Mammal\"])\n",
    "\n",
    "for a in classes:\n",
    "    filtering = list(map(lambda x: (int(x[0][2]), round(x[1])), filter(lambda x : x[0][1] == a, conca)))\n",
    "    filtering = [list(t) for t in zip(*filtering)]\n",
    "    print(a, f1_score(filtering[0], filtering[1]))\n",
    "filtering = list(map(lambda x: (int(x[0][2]), round(x[1])), filter(lambda x : x[0][1] != \"asfa\", conca)))\n",
    "filtering = [list(t) for t in zip(*filtering)]\n",
    "print(\"General\", f1_score(filtering[0], filtering[1]))\n",
    "print(\"Precision\", precision_score(filtering[0], filtering[1]))\n",
    "print(\"Recall\", recall_score(filtering[0], filtering[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "classes = set([\"Plant\", \"Eukaryote\", \"Species\", \"Fungus\", \"Bacteria\", \"Animal\", \"Mammal\"])\n",
    "\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n",
    "vocab_size = len(classes)  # gives the total number of unique words\n",
    "\n",
    "for i, word in enumerate(classes):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    \n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them to numpy arrays\n",
    "train_clazz = []  # input word\n",
    "entity_one_train = []  # input word\n",
    "y_train = []  # output word\n",
    "\n",
    "for a,b,c in training:\n",
    "    clazz = to_one_hot(word2int[b], vocab_size)\n",
    "    second = tee[a][100:]\n",
    "    \n",
    "    entity_one_train.append(second)\n",
    "    train_clazz.append(clazz)\n",
    "\n",
    "    y_train.append([c])\n",
    "\n",
    "\n",
    "\n",
    "train_clazz = np.asarray(train_clazz)\n",
    "entity_one_train = np.asarray(entity_one_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them to numpy arrays\n",
    "test_clazz = []  # input word\n",
    "entity_one_test = []  # input word\n",
    "\n",
    "y_test = []  # output word\n",
    "\n",
    "for a,b,c in testing:\n",
    "    clazz = to_one_hot(word2int[b], vocab_size)\n",
    "    second = tee[a][100:]\n",
    "    \n",
    "    entity_one_test.append(second)\n",
    "    test_clazz.append(clazz)\n",
    "\n",
    "    y_test.append([c])\n",
    "\n",
    "\n",
    "\n",
    "test_clazz = np.asarray(test_clazz)\n",
    "entity_one_test = np.asarray(entity_one_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers, regularizers\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "entity_vocabulary_size = len(entities_to_train_on)\n",
    "\n",
    "prop_vocabulary_size = 7\n",
    "\n",
    "entity_one_input = Input(shape=(100,))\n",
    "entity_one_first_step = layers.Dense(20, activation=\"relu\")(entity_one_input)\n",
    "\n",
    "entity_two_input = Input(shape=(prop_vocabulary_size,))\n",
    "entity_two_first_step = layers.Dense(20, activation=\"relu\")(entity_two_input)\n",
    "\n",
    "\n",
    "concatenated = layers.concatenate([entity_one_first_step, entity_two_first_step], axis=-1)\n",
    "\n",
    "belta = layers.Dense(20, activation=\"relu\")(concatenated)\n",
    "dropout = layers.Dropout(0.2)(belta)\n",
    "answer = layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model = Model([entity_one_input, entity_two_input], answer)\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[ \n",
    "                                                                        keras_metrics.precision(), \n",
    "                                                                        keras_metrics.recall(),\n",
    "                                                                        keras_metrics.f1_score()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([entity_one_train,train_clazz], y_train, epochs=3000, batch_size=500,  callbacks =[es], validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dada = (model.evaluate([entity_one_test,test_clazz], y_test))\n",
    "\n",
    "print(dada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dnn = model.predict([entity_one_test,test_clazz])\n",
    "res_dnn_rounded = list(map(lambda x : round(x[0]), res_dnn))\n",
    "zipped_test_results_collection = list(zip(testing, res_dnn_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "classes = set([\"Plant\", \"Eukaryote\", \"Species\", \"Fungus\", \"Bacteria\", \"Animal\", \"Mammal\"])\n",
    "\n",
    "for a in classes:\n",
    "    filtering = list(map(lambda x: (int(x[0][2]), round(x[1])), filter(lambda x : x[0][1] == a, zipped_test_results_collection)))\n",
    "    filtering = [list(t) for t in zip(*filtering)]\n",
    "    print(a, f1_score(filtering[0], filtering[1]))\n",
    "filtering = list(map(lambda x: (int(x[0][2]), round(x[1])), filter(lambda x : x[0][1] != \"asfa\", zipped_test_results_collection)))\n",
    "filtering = [list(t) for t in zip(*filtering)]\n",
    "print(\"General\", f1_score(filtering[0], filtering[1]))\n",
    "print(\"Precision\", precision_score(filtering[0], filtering[1]))\n",
    "print(\"Recall\", recall_score(filtering[0], filtering[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
